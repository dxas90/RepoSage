{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6e4e83-016a-4dae-b4fb-482c5ebcd1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! uv pip install jupyter requests qdrant-client llama-index llama-index-embeddings-huggingface llama-index-vector-stores-qdrant llama-index-llms-ollama watchdog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca94dd9-7755-47e1-b902-ccabe019f938",
   "metadata": {},
   "source": [
    "# Get the sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fa624b-615d-4171-803e-d3c21e74c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to your sources.yaml file\n",
    "yaml_file = Path(\"sources.yaml\")\n",
    "\n",
    "# Load YAML file\n",
    "with yaml_file.open(\"r\") as f:\n",
    "    sources = yaml.safe_load(f)\n",
    "\n",
    "# Base directory for all repos\n",
    "base_dir = Path(\"..\") / \"markdown_docs\"\n",
    "\n",
    "for key, repos in sources.items():\n",
    "    dir_path = base_dir / key\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"\\nüìÅ Directory: {dir_path}\")\n",
    "\n",
    "    for repo in repos:\n",
    "        repo_name = Path(repo).stem.replace(\".git\", \"\")\n",
    "        target_dir = dir_path / repo_name\n",
    "\n",
    "        if target_dir.exists() and (target_dir / \".git\").exists():\n",
    "            print(f\"üîÑ Updating existing repo: {repo_name}\")\n",
    "            subprocess.run(\n",
    "                [\"git\", \"pull\", \"--ff-only\"],\n",
    "                cwd=target_dir,\n",
    "                check=False\n",
    "            )\n",
    "        else:\n",
    "            print(f\"‚¨áÔ∏è Cloning new repo: {repo_name}\")\n",
    "            subprocess.run(\n",
    "                [\"git\", \"clone\", \"--depth=1\", repo],\n",
    "                cwd=dir_path,\n",
    "                check=True\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6e885f-1d8d-4cfe-bf80-853f0ab93095",
   "metadata": {},
   "source": [
    "# load the markdowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b945e-2476-44c3-8cbd-be115ae2b060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß© Local RAG Question Answering with Ollama + Qdrant + Markdown files\n",
    "\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    Settings,\n",
    ")\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from qdrant_client import QdrantClient, models\n",
    "import requests\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def ensure_model(model_name: str = \"llama3.2:latest\", host: str = \"http://localhost:11434\", allow_pull: bool = True):\n",
    "    \"\"\"Ensure the given Ollama model exists locally.\"\"\"\n",
    "    try:\n",
    "        resp = requests.get(f\"{host}/api/tags\")\n",
    "        resp.raise_for_status()\n",
    "        tags = [m[\"name\"] for m in resp.json().get(\"models\", [])]\n",
    "        if model_name in tags:\n",
    "            print(f\"‚úÖ Model '{model_name}' found locally.\")\n",
    "            return True\n",
    "        elif allow_pull:\n",
    "            print(f\"‚¨áÔ∏è Pulling '{model_name}' from Ollama registry...\")\n",
    "            requests.post(f\"{host}/api/pull\", json={\"name\": model_name})\n",
    "            print(f\"‚úÖ Model '{model_name}' pulled successfully.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Model '{model_name}' not found and pulling disabled.\")\n",
    "            return False\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"‚ö†Ô∏è Ollama API not reachable at\", host)\n",
    "        return False\n",
    "\n",
    "\n",
    "# üß† Configuration\n",
    "DATA_DIR = \"../markdown_docs\"\n",
    "COLLECTION_NAME = \"markdown_rag\"\n",
    "QDRANT_HOST = \"localhost\"\n",
    "QDRANT_PORT = 6333\n",
    "\n",
    "# 1Ô∏è‚É£ Connect to Qdrant (persistent volume inside docker)\n",
    "qdrant = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)\n",
    "\n",
    "# 2Ô∏è‚É£ Define embedding + LLM models\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "ensure_model(\"llama3.2:latest\")\n",
    "llm = Ollama(model=\"llama3.2:latest\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "# 3Ô∏è‚É£ Global LlamaIndex settings\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm\n",
    "\n",
    "# 4Ô∏è‚É£ Load Markdown documents recursively\n",
    "print(\"üìÇ Loading markdown files...\")\n",
    "docs = SimpleDirectoryReader(\n",
    "    input_dir=DATA_DIR,\n",
    "    required_exts=[\".md\", \".yml\", \".yaml\"],\n",
    "    recursive=True\n",
    ").load_data()\n",
    "\n",
    "# Add full file paths to metadata\n",
    "for doc in docs:\n",
    "    if \"file_path\" not in doc.metadata:\n",
    "        doc.metadata[\"file_path\"] = os.path.abspath(doc.metadata.get(\"file_name\", \"\"))\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(docs)} markdown files.\")\n",
    "\n",
    "# 5Ô∏è‚É£ Ensure Qdrant collection exists with correct vector size\n",
    "embedding_dim = len(embed_model.get_text_embedding(\"test\"))\n",
    "\n",
    "try:\n",
    "    qdrant.get_collection(COLLECTION_NAME)\n",
    "    print(f\"‚úÖ Using existing Qdrant collection: {COLLECTION_NAME}\")\n",
    "except Exception:\n",
    "    print(f\"üÜï Creating new Qdrant collection: {COLLECTION_NAME}\")\n",
    "    qdrant.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=models.VectorParams(size=embedding_dim, distance=models.Distance.COSINE),\n",
    "    )\n",
    "\n",
    "# 6Ô∏è‚É£ Create / reuse vector index using Qdrant\n",
    "vector_store = QdrantVectorStore(client=qdrant, collection_name=COLLECTION_NAME)\n",
    "index = VectorStoreIndex.from_documents(docs, vector_store=vector_store)\n",
    "\n",
    "# 7Ô∏è‚É£ Query engine setup\n",
    "query_engine = index.as_query_engine(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f661a73-6a5d-4d57-a886-130f1827fce5",
   "metadata": {},
   "source": [
    "# query the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16010541-7fd9-4a5d-b894-d40a1c1384f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8Ô∏è‚É£ Ask a question\n",
    "\n",
    "question = \"\"\"\n",
    "gimme a full github action implementation for a erlang app\n",
    "\"\"\"\n",
    "\n",
    "response = query_engine.query(question)\n",
    "\n",
    "print(\"üß† Answer:\\n\", response.response)\n",
    "print(\"\\nüìÑ Sources:\")\n",
    "for src in response.source_nodes:\n",
    "    print(\"-\", src.metadata.get(\"file_path\", \"unknown\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d0f1de-7658-43e1-8afd-04a69835bb78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
